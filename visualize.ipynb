{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import config\n",
    "\n",
    "from model_structure import UNet\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PyTorch model\n",
    "model = UNet()\n",
    "model.load_state_dict(torch.load(\"./model_for_vasc.pth\", map_location=\"cpu\"))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Directory paths for input and output data\n",
    "input_dir = \"./preparations/data/indata\"\n",
    "output_dir = \"./preparations/data/outdata\"\n",
    "\n",
    "# Get a list of image files in the input directory\n",
    "image_files = os.listdir(input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atashitk\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\functional.py:1352: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.\n",
      "  warnings.warn(\"dropout2d: Received a 3D input to dropout2d and assuming that channel-wise \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4095\n",
      "(512, 512)\n",
      "0.984619140625\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Choose a random pair of input and output files\n",
    "    random_image_file = random.choice(image_files)\n",
    "\n",
    "    # Construct the full file paths\n",
    "    input_image_path = os.path.join(input_dir, random_image_file)\n",
    "    output_label_path = os.path.join(output_dir, random_image_file)\n",
    "    \n",
    "    label_image = cv2.imread(output_label_path, cv2.IMREAD_GRAYSCALE)\n",
    "    shape = label_image.shape\n",
    "    random_x, random_y = random.randint(0, shape[0] - config.WIDTH - 1), random.randint(0, shape[1] - config.HEIGHT - 1)\n",
    "    label_image = label_image[random_y : random_y + config.HEIGHT, random_x : random_x + config.WIDTH]\n",
    "    if cv2.countNonZero(label_image) == 0:\n",
    "        continue\n",
    "    input_image = cv2.imread(input_image_path, cv2.IMREAD_GRAYSCALE)[random_y : random_y + config.HEIGHT, random_x : random_x + config.WIDTH]\n",
    "    break\n",
    "\n",
    "# Perform inference using the model\n",
    "with torch.no_grad():\n",
    "    input_tensor = torch.from_numpy(input_image / 255).unsqueeze(0).float()\n",
    "    output = model(input_tensor)\n",
    "# Convert the output tensor to a NumPy array\n",
    "output_np = output.numpy()\n",
    "# Convert the output prediction to binary format and multiply by 255\n",
    "print((np.unique(output_np).size))\n",
    "binary_output = (output_np >= 0.5).astype(np.uint8)[0] * 255\n",
    "# Display the input image, label, and binary prediction using OpenCV\n",
    "cv2.imshow(\"Input Image\", cv2.resize(input_image, (512, 512)))\n",
    "cv2.imshow(\"Label Image\", cv2.resize(label_image, (512, 512)))\n",
    "print(cv2.resize(binary_output, (512, 512)).shape)\n",
    "cv2.imshow(\"Model Prediction (Binary)\", cv2.resize(binary_output, (512, 512)))\n",
    "cv2.imshow(\"Correctness Image\", cv2.resize((np.multiply(binary_output, label_image) + np.multiply((255 - binary_output), (255 - label_image))) * 255, (512, 512)))\n",
    "\n",
    "print(np.sum(np.multiply(binary_output, label_image) + np.multiply((255 - binary_output), (255 - label_image))) / 64 / 64)\n",
    "\n",
    "# Wait for a key press and then close the OpenCV windows\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(img, margin):\n",
    "    if margin != 0:\n",
    "        raise ValueError\n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.from_numpy(img / 255).unsqueeze(0).float()\n",
    "        output = model(input_tensor)\n",
    "    output_np = output.numpy()\n",
    "    return output_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/41 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atashitk\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\functional.py:1352: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.\n",
      "  warnings.warn(\"dropout2d: Received a 3D input to dropout2d and assuming that channel-wise \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2.6749704881701142e-11 1.0 \n",
      "\n",
      "\n",
      " 3.851931187209834e-11 2.0 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   2%|▏         | 1/41 [00:00<00:15,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 5.360549070604348e-11 3.0 \n",
      "\n",
      "\n",
      " 6.469011697846083e-11 4.0 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 41/41 [00:12<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000000e+00 1.26865281e-15 1.34204641e-15 ... 1.99849343e+00\n",
      " 1.99883956e+00 1.99890262e+00]\n",
      "[ 0  3  4 16 17 18 19 20 21]\n",
      "817.859375\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"./preparations/data/indata\"\n",
    "output_dir = \"./preparations/data/outdata\"\n",
    "\n",
    "# Get a list of image files in the input directory\n",
    "image_files = os.listdir(input_dir)\n",
    "\n",
    "random_image_file = random.choice(image_files)\n",
    "input_image_path = os.path.join(input_dir, random_image_file)\n",
    "output_label_path = os.path.join(output_dir, random_image_file)\n",
    "\n",
    "input_image = cv2.imread(input_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "label_image = cv2.imread(output_label_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "shape = label_image.shape\n",
    "count_times = np.zeros(shape)\n",
    "total_times = np.zeros(shape)\n",
    "\n",
    "margin = 0\n",
    "step = 32\n",
    "arr1 = []\n",
    "arr2 = []\n",
    "for i in tqdm(range(0, shape[0] - config.HEIGHT, step), \"Processing\"):\n",
    "    for j in range(0, shape[1] - config.WIDTH, step):\n",
    "        slice = input_image[i : i + config.HEIGHT, j : j + config.WIDTH]\n",
    "        # print(i, j, slice.shape)\n",
    "        array_tmp = get_prediction(slice, margin)[0]\n",
    "        # print(array_tmp.shape)\n",
    "        count_times[i : (i + config.HEIGHT), j : (j + config.WIDTH)] += array_tmp\n",
    "        \n",
    "        total_times[i : (i + config.HEIGHT), j : (j + config.WIDTH)] += 1\n",
    "        if i <= 63 and j <= 63:\n",
    "            print(\"\\n\", count_times[48][48], total_times[48][48], \"\\n\")\n",
    "        \n",
    "\n",
    "confidence = count_times / config.WIDTH * step\n",
    "print(np.unique(confidence))\n",
    "\n",
    "binary_output = (np.floor(confidence * 100 * 255)).astype(np.uint8)[0]\n",
    "print(np.unique(binary_output))\n",
    "\n",
    "cv2.imshow(\"Input Image\", cv2.resize(input_image, (512, 512)))\n",
    "cv2.imshow(\"Label Image\", cv2.resize(label_image, (512, 512)))\n",
    "cv2.imshow(\"Model Prediction (Binary)\", cv2.resize(binary_output, (512, 512)))\n",
    "cv2.imshow(\"Correctness Image\", cv2.resize((np.multiply(binary_output, label_image) + np.multiply((255 - binary_output), (255 - label_image))) * 255, (512, 512)))\n",
    "\n",
    "print(np.sum(np.multiply(binary_output, label_image) + np.multiply((255 - binary_output), (255 - label_image))) / 64 / 64)\n",
    "\n",
    "# Wait for a key press and then close the OpenCV windows\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003219530453743247\n",
      "0.9145284842290359\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
