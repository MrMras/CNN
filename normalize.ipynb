{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import concurrent.futures\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findBordersHorizontal(img, border, side):\n",
    "    if side == \"l2r\":\n",
    "        for i in range(border):\n",
    "            if np.sum(img[:,i]) > 0:\n",
    "                return i\n",
    "    elif side == \"r2l\":\n",
    "        for i in range(img.shape[1] - 1, border, -1):\n",
    "            if np.sum(img[:,i]) > 0:\n",
    "                return i\n",
    "    else:\n",
    "        raise ValueError\n",
    "    return border\n",
    "\n",
    "def findBordersVertical(img, border, side):\n",
    "    if side == \"t2b\":\n",
    "        for i in range(border):\n",
    "            if np.sum(img[i]) > 0:\n",
    "                return i\n",
    "    elif side == \"b2t\":\n",
    "        for i in range(img.shape[0] - 1, border, -1):\n",
    "            if np.sum(img[i]) > 0:\n",
    "                return i\n",
    "    else:\n",
    "        raise ValueError\n",
    "    return border\n",
    "\n",
    "\n",
    "def findBorders(img, bordLeft, bordRight, bordTop, bordBottom):\n",
    "    bordLeft = findBordersHorizontal(img, bordLeft, \"l2r\")\n",
    "    bordRight = findBordersHorizontal(img, bordRight, \"r2l\")\n",
    "    bordTop = findBordersVertical(img, bordTop, \"t2b\")\n",
    "    bordBottom = findBordersVertical(img, bordBottom, \"b2t\")\n",
    "    return bordLeft, bordRight, bordTop, bordBottom\n",
    "\n",
    "\n",
    "# Function to find borders for a single image\n",
    "def find_borders(img_path):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    return findBorders(img, img.shape[0] - 1, 0, img.shape[1] - 1, 0)\n",
    "\n",
    "# Function to merge borders from multiple images\n",
    "def merge_borders(borders):\n",
    "    bordLeft, bordRight, bordTop, bordBottom = borders[0]\n",
    "    for b in borders[1:]:\n",
    "        bordLeftTmp, bordRightTmp, bordTopTmp, bordBottomTmp = b\n",
    "        bordLeft = min(bordLeft, bordLeftTmp)\n",
    "        bordRight = max(bordRight, bordRightTmp)\n",
    "        bordTop = min(bordTop, bordTopTmp)\n",
    "        bordBottom = max(bordBottom, bordBottomTmp)\n",
    "    return bordLeft, bordRight, bordTop, bordBottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # path to the folder with already segmented data\n",
    "# path_folder = \"./preparations/data/outdata/\"\n",
    "\n",
    "# # get a list of paths to the files\n",
    "# dirs = os.listdir(path_folder)\n",
    "# path_files = [os.path.join(path_folder, x) for x in dirs]\n",
    "\n",
    "# # Number of threads to use\n",
    "# num_threads = min(len(path_files), 4)  # Adjust the number of threads as needed\n",
    "\n",
    "# borders = []\n",
    "\n",
    "# # Use ThreadPoolExecutor to parallelize the finding of borders\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "#     # Submit tasks for each image to find borders concurrently\n",
    "#     futures = [executor.submit(find_borders, path) for path in path_files]\n",
    "\n",
    "#     # Collect results\n",
    "#     for future in tqdm(concurrent.futures.as_completed(futures), \"Finding borders\"):\n",
    "#         borders.append(future.result())\n",
    "\n",
    "# # Merge borders from multiple images\n",
    "# bordLeft, bordRight, bordTop, bordBottom = merge_borders(borders)\n",
    "\n",
    "# print(bordLeft, bordRight, bordTop, bordBottom)\n",
    "\n",
    "# horizontal = bordRight - bordLeft\n",
    "# vertical = bordBottom - bordTop\n",
    "\n",
    "# print(\"Initial shape: ({0}, {1})\".format(vertical, horizontal))\n",
    "\n",
    "# horizontalExtra = horizontal % config.WIDTH\n",
    "# verticalExtra = vertical % config.HEIGHT\n",
    "\n",
    "# print(\"Extras : ({0}, {1})\".format(verticalExtra, horizontalExtra))\n",
    "\n",
    "# bordLeft = int(bordLeft + horizontalExtra / 2)\n",
    "# bordRight = int(bordRight - horizontalExtra / 2)\n",
    "\n",
    "# bordTop = int(bordTop + verticalExtra / 2)\n",
    "# bordBottom = int(bordBottom - verticalExtra / 2)\n",
    "\n",
    "# horizontal = bordRight - bordLeft\n",
    "# vertical = bordBottom - bordTop\n",
    "\n",
    "# print(\"New shape: ({0}, {1})\".format(vertical, horizontal))\n",
    "\n",
    "vertical = 384\n",
    "horizontal = 384\n",
    "bordTop = 0\n",
    "bordLeft = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_data(in_path1, in_path2, out_path1, out_path2, borders):\n",
    "    # Extracting border coordinates\n",
    "    bordTop, bordLeft = borders\n",
    "    # Starting index for saving images\n",
    "    ind = 1000000\n",
    "    # List of files in input paths\n",
    "    files_in1 = os.listdir(in_path1)\n",
    "    files_in2 = os.listdir(in_path2)\n",
    "    # List to store mean intensities of processed images\n",
    "    sizes = np.array([])\n",
    "\n",
    "    # Calculating the number of splits vertically and horizontally\n",
    "    vertical_split = vertical // config.HEIGHT\n",
    "    horizontal_split = horizontal // config.WIDTH\n",
    "         \n",
    "    # Calculating grid size\n",
    "    grid_size = vertical_split * horizontal_split\n",
    "    \n",
    "    for i in tqdm(range(vertical_split * horizontal_split), desc=\"Processing\"):\n",
    "        # Calculate the grid indices\n",
    "        j = i % horizontal_split\n",
    "        k = i // horizontal_split\n",
    "\n",
    "        # Iterate through the image files in batches\n",
    "        for i in range(0, len(files_in2) - len(files_in2) % config.NUM_PICS, config.NUM_PICS):\n",
    "            # Initialize a list to store intensities for each image in the group\n",
    "            group_intensities = np.array([])\n",
    "\n",
    "            # Iterate through the images in the current group\n",
    "            for l, x in enumerate(files_in2[i:i+config.NUM_PICS]):\n",
    "                # Read and extract a region of interest from the image\n",
    "                img_tmp1 = cv2.imread(os.path.join(in_path1, x), cv2.IMREAD_GRAYSCALE)\n",
    "                img_tmp1 = img_tmp1[bordTop + config.HEIGHT * k: bordTop + config.HEIGHT * (k + 1), bordLeft + config.WIDTH * j: bordLeft + config.WIDTH * (j + 1)]\n",
    "                # Calculate and store the mean intensity of the current image\n",
    "                group_intensities = np.append(group_intensities, np.mean(img_tmp1))\n",
    "\n",
    "            # Calculate the average intensity for the current group\n",
    "            average_intensity = np.mean(group_intensities)\n",
    "\n",
    "            # Store the average intensity in the 'sizes' list\n",
    "            sizes = np.append(sizes, average_intensity)\n",
    "\n",
    "    # Calculating the median of the mean intensities\n",
    "    print(sizes.dtype)\n",
    "    non_zero_values = sizes[sizes != 0]\n",
    "    print(\"Uniques:\", np.unique(sizes))\n",
    "    med = np.median(non_zero_values)\n",
    "    print(\"Median\", med)\n",
    "    mean = np.mean(non_zero_values)\n",
    "    print(\"Mean\", mean)\n",
    "    # print(f\"length - {len(files_in1)}\")\n",
    "    # print(f\"length of sizes - {len(sizes)}\")\n",
    "    # print(f\"grid_size - {grid_size}\")\n",
    "    # Iterating through each set of images and writing selected ones\n",
    "    for i in tqdm(range(len(sizes)), \"Writting images\"):\n",
    "        # Checking if the mean intensity of the set is greater than or equal to the median\n",
    "        if sizes[i] >= med:\n",
    "            # Extracting indices for the selected set of images\n",
    "            pic_number = [(i * config.NUM_PICS + j) % (len(files_in1) -  len(files_in1) % config.NUM_PICS) for j in range(config.NUM_PICS)]\n",
    "            horizontal_number = i // ((len(files_in1) -  len(files_in1) % config.NUM_PICS) // config.NUM_PICS) % horizontal_split\n",
    "            vertical_number = i // ((len(files_in1) -  len(files_in1) % config.NUM_PICS) // config.NUM_PICS) // horizontal_split\n",
    "\n",
    "            # Iterating through images in the selected set and writing them\n",
    "            for k in range(config.NUM_PICS):\n",
    "                \n",
    "                # print(f\"i - {i}\\nk - {k}\\npic_number[k] - {pic_number[k]}\")\n",
    "                img_tmp1 = cv2.imread(os.path.join(in_path1, files_in1[pic_number[k]]), cv2.IMREAD_GRAYSCALE)\n",
    "                img_tmp2 = cv2.imread(os.path.join(in_path2, files_in2[pic_number[k]]), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "                img_tmp1 = img_tmp1[bordTop  + config.HEIGHT * vertical_number: bordTop  + config.HEIGHT * (vertical_number + 1), bordLeft + config.WIDTH * horizontal_number : bordLeft + config.WIDTH * (horizontal_number + 1)]\n",
    "                img_tmp2 = img_tmp2[bordTop  + config.HEIGHT * vertical_number: bordTop  + config.HEIGHT * (vertical_number + 1), bordLeft + config.WIDTH * horizontal_number : bordLeft + config.WIDTH * (horizontal_number + 1)]\n",
    "                cv2.imwrite(os.path.join(out_path1, f\"img_{ind}_{k}.png\"), img_tmp1)\n",
    "                cv2.imwrite(os.path.join(out_path2, f\"img_{ind}_{k}.png\"), img_tmp2)\n",
    "            ind += 1\n",
    "\n",
    "\n",
    "def cut_data_2(data_in, data_out, out_path1, out_path2, borders):\n",
    "    # Extracting border coordinates\n",
    "    bordTop, bordLeft = borders\n",
    "    # Starting index for saving images\n",
    "    ind = 1000000\n",
    "    # List of files in input paths\n",
    "    # List to store mean intensities of processed images\n",
    "    sizes = np.array([])\n",
    "\n",
    "    # Calculating the number of splits vertically and horizontally\n",
    "    vertical_split = vertical // config.HEIGHT\n",
    "    horizontal_split = horizontal // config.WIDTH\n",
    "         \n",
    "    # Calculating grid size\n",
    "    grid_size = vertical_split * horizontal_split\n",
    "    shape = data_in.shape\n",
    "\n",
    "    for i in tqdm(range(vertical_split * horizontal_split), desc=\"Processing\"):\n",
    "        # Calculate the grid indices\n",
    "        j = i % horizontal_split\n",
    "        k = i // horizontal_split\n",
    "\n",
    "        # Iterate through the image files in batches\n",
    "        for i in range(0, shape[2] - shape[2] % config.NUM_PICS, config.NUM_PICS):\n",
    "            # Initialize a list to store intensities for each image in the group\n",
    "            group_intensities = np.array([])\n",
    "\n",
    "            # Iterate through the images in the current group\n",
    "            for i in range(shape[2]):\n",
    "                # Read and extract a region of interest from the image\n",
    "                img_tmp1 = data_out[bordTop + config.HEIGHT * k: bordTop + config.HEIGHT * (k + 1), bordLeft + config.WIDTH * j: bordLeft + config.WIDTH * (j + 1), i]\n",
    "                # Calculate and store the mean intensity of the current image\n",
    "                group_intensities = np.append(group_intensities, np.mean(img_tmp1))\n",
    "\n",
    "            # Calculate the average intensity for the current group\n",
    "            average_intensity = np.mean(group_intensities)\n",
    "\n",
    "            # Store the average intensity in the 'sizes' list\n",
    "            sizes = np.append(sizes, average_intensity)\n",
    "\n",
    "    # Calculating the median of the mean intensities\n",
    "    print(sizes.dtype)\n",
    "    non_zero_values = sizes[sizes != 0]\n",
    "    print(\"Uniques:\", np.unique(sizes))\n",
    "    med = np.median(non_zero_values)\n",
    "    print(\"Median\", med)\n",
    "    mean = np.mean(non_zero_values)\n",
    "    print(\"Mean\", mean)\n",
    "    # print(f\"length - {len(files_in1)}\")\n",
    "    # print(f\"length of sizes - {len(sizes)}\")\n",
    "    # print(f\"grid_size - {grid_size}\")\n",
    "    # Iterating through each set of images and writing selected ones\n",
    "    for i in tqdm(range(len(sizes)), \"Writting images\"):\n",
    "        # Checking if the mean intensity of the set is greater than or equal to the median\n",
    "        if sizes[i] >= med:\n",
    "            # Extracting indices for the selected set of images\n",
    "            pic_number = [(i * config.NUM_PICS + j) % (shape[2] - shape[2] % config.NUM_PICS) for j in range(config.NUM_PICS)]\n",
    "            horizontal_number = i // ((shape[2] - shape[2] % config.NUM_PICS) // config.NUM_PICS) % horizontal_split\n",
    "            vertical_number = i // ((shape[2] - shape[2] % config.NUM_PICS) // config.NUM_PICS) // horizontal_split\n",
    "\n",
    "            # Iterating through images in the selected set and writing them\n",
    "            for k in range(config.NUM_PICS):\n",
    "                \n",
    "                # print(f\"i - {i}\\nk - {k}\\npic_number[k] - {pic_number[k]}\")\n",
    "                img_tmp1 = data_out[:, :, pic_number[k]]\n",
    "                img_tmp2 = data_in[:, :, pic_number[k]]\n",
    "\n",
    "                img_tmp1 = img_tmp1[bordTop  + config.HEIGHT * vertical_number: bordTop  + config.HEIGHT * (vertical_number + 1), bordLeft + config.WIDTH * horizontal_number : bordLeft + config.WIDTH * (horizontal_number + 1)]\n",
    "                img_tmp2 = img_tmp2[bordTop  + config.HEIGHT * vertical_number: bordTop  + config.HEIGHT * (vertical_number + 1), bordLeft + config.WIDTH * horizontal_number : bordLeft + config.WIDTH * (horizontal_number + 1)]\n",
    "                cv2.imwrite(os.path.join(out_path1, f\"img_{ind}_{k}.png\"), img_tmp1)\n",
    "                cv2.imwrite(os.path.join(out_path2, f\"img_{ind}_{k}.png\"), img_tmp2)\n",
    "            ind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the directory path\n",
    "# base_dir = './dataset'\n",
    "\n",
    "# # Create the 'dataset' folder\n",
    "# if not os.path.exists(base_dir):\n",
    "#     os.makedirs(base_dir)\n",
    "\n",
    "# # Create the 'indata' folder inside 'dataset'\n",
    "# indata_dir = os.path.join(base_dir, 'indata')\n",
    "# if not os.path.exists(indata_dir):\n",
    "#     os.makedirs(indata_dir)\n",
    "\n",
    "# # Create the 'outdata' folder inside 'dataset'\n",
    "# outdata_dir = os.path.join(base_dir, 'outdata')\n",
    "# if not os.path.exists(outdata_dir):\n",
    "#     os.makedirs(outdata_dir)\n",
    "\n",
    "# path_folder1 = \"./preparations/data/outdata/\"\n",
    "# path_folder1_cut = \"./dataset/outdata/\"\n",
    "\n",
    "# path_folder2 = \"./preparations/data/indata/\"\n",
    "# path_folder2_cut = \"./dataset/indata/\" \n",
    "\n",
    "# cut_data(path_folder1, path_folder2, path_folder1_cut, path_folder2_cut, (bordTop, bordLeft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 36/36 [01:32<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "Uniques: [0.01664404 0.01766089 0.01875537 0.01903833 0.01955811 0.01977393\n",
      " 0.01997827 0.02010278 0.02014014 0.02042578 0.02100537 0.02202173\n",
      " 0.02213013 0.02250415 0.0228208  0.02303809 0.02362817 0.02511035\n",
      " 0.02511768 0.02547241 0.02571899 0.02631372 0.02649316 0.02697095\n",
      " 0.02711914 0.02826587 0.02838208 0.02847144 0.02868066 0.02973511\n",
      " 0.0297749  0.0323103  0.03535229 0.04430859 0.04712671 0.04968896]\n",
      "Median 0.025114013671875\n",
      "Mean 0.026101094563802083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writting images: 100%|██████████| 2232/2232 [00:17<00:00, 129.81it/s]\n"
     ]
    }
   ],
   "source": [
    "data_in = np.load(\"./KESM/whole_volume_kesm.npy\")\n",
    "data_out = np.load(\"./KESM/ground_truth_kesm.npy\")\n",
    "base_dir = './dataset_kesm'\n",
    "path_folder1_cut = \"./dataset_kesm/outdata/\"\n",
    "path_folder2_cut = \"./dataset_kesm/indata/\"\n",
    "\n",
    "# Create the 'dataset' folder\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)\n",
    "\n",
    "# Create the 'indata' folder inside 'dataset'\n",
    "indata_dir = os.path.join(base_dir, 'indata')\n",
    "if not os.path.exists(indata_dir):\n",
    "    os.makedirs(indata_dir)\n",
    "\n",
    "# Create the 'outdata' folder inside 'dataset'\n",
    "outdata_dir = os.path.join(base_dir, 'outdata')\n",
    "if not os.path.exists(outdata_dir):\n",
    "    os.makedirs(outdata_dir)\n",
    "\n",
    "\n",
    "cut_data_2(data_in, data_out, path_folder1_cut, path_folder2_cut, (bordTop, bordLeft))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
